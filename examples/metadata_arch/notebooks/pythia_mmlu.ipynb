{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4a0b667-c53d-4b2e-af39-1920249215f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d97f4d-c01a-45dd-8b6f-4827fcfe9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_task_list =  ['abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48ac3192-7262-4efe-aedb-d452b9ea00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high school + 'elementary' tasks\n",
    "mmlu_hs_list = [task for task in mmlu_task_list if task.startswith('high') or task.startswith('elementary')]\n",
    "# college + professional tasks\n",
    "mmlu_adv_list = [task for task in mmlu_task_list if task.startswith('college') or task.startswith('professional')]\n",
    "\n",
    "mmlu_hs_dict = {task:None for task in mmlu_hs_list}\n",
    "mmlu_adv_dict = {task:None for task in mmlu_adv_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f53e5d-b79b-4237-b687-85cd30799efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load mmlu\n",
    "for task in mmlu_hs_list:\n",
    "    mmlu_hs_dict[task] = load_dataset('cais/mmlu', task)\n",
    "\n",
    "for task in mmlu_adv_list:\n",
    "    mmlu_adv_dict[task] = load_dataset('cais/mmlu', task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64bc55b7-e7d0-4939-ab7f-1b19a68ed4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge validation splits\n",
    "hs_val = concatenate_datasets([datadict['validation'] for _, datadict in mmlu_hs_dict.items()])\n",
    "adv_val = concatenate_datasets([datadict['validation'] for _, datadict in mmlu_adv_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19e6ff23-4b69-4b7c-a998-eae7b1a5c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "num_samples = 350\n",
    "seed = 42\n",
    "\n",
    "hs_val = hs_val.shuffle(seed=seed).select(list(range(num_samples)))\n",
    "adv_val = adv_val.shuffle(seed=seed).select(list(range(num_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea6bdf3e-e075-4667-9ddf-9eb26566ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval on pythia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c17f3-eb73-4962-98d7-68ea0313f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "# Two sets of eight models of sizes 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. \n",
    "# For each size, there are two models: one trained on the Pile, \n",
    "# and one trained on the Pile after the dataset has been globally deduplicated.\n",
    "# 143 evenly-spaced checkpoints from step1000 to step143000\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-70m-deduped/step3000\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-70m-deduped/step3000\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\n",
    "tokens = model.generate(**inputs)\n",
    "tokenizer.decode(tokens[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
