{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA1gNyEf5Jr5"
   },
   "source": [
    "## How to contribute by creating a new task?\n",
    "\n",
    "In this notebook, we will do a quick walkthrough of how to use `lm-evaluation-harness` in order to implement a new task from scratch.\n",
    "\n",
    "The guide is based on the [original guide for adding a new task](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md) - feel free to check out that guide for a more comprehensive tutorial. In this tutorial, we will create a new small variant of `piqa` benchmark to augment it with some paraphrase tool (only for demonstration purpose).\n",
    "\n",
    "\n",
    "Make sure to [first fork the original repository](https://github.com/EleutherAI/lm-evaluation-harness.git) and clone it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajzK1ww-SWoA",
    "outputId": "278d085e-39e9-4899-9aeb-9ce02c252772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADk5razW5uf5"
   },
   "source": [
    "We will clone and build the package from the main repository but you should clone your fork here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCG38ndt5qZx",
    "outputId": "ee7b6904-7ca3-4b9e-e316-ee0dbb9d2da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lm-evaluation-harness'...\n",
      "remote: Enumerating objects: 49693, done.\u001b[K\n",
      "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
      "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
      "remote: Total 49693 (delta 29), reused 18 (delta 15), pack-reused 49641 (from 2)\u001b[K\n",
      "Receiving objects: 100% (49693/49693), 29.64 MiB | 9.65 MiB/s, done.\n",
      "Resolving deltas: 100% (34403/34403), done.\n",
      "/content/lm-evaluation-harness\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.7/220.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.0/244.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building editable for lm_eval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m/content\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\n",
    "%cd lm-evaluation-harness\n",
    "!pip install -e \".[dev]\" -q\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0gU_VkB6YW_"
   },
   "source": [
    "Now, we will navigate within the repo and create a new YAML file [as per the documentation](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#creating-a-yaml-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBe2xqlCVP3E",
    "outputId": "ae2a7fed-fa19-4cb0-b805-c80cb8e92c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/lm-evaluation-harness\n"
     ]
    }
   ],
   "source": [
    "%cd lm-evaluation-harness\n",
    "!cp -r templates/new_yaml_task lm_eval/tasks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo5EV0fHH5RZ"
   },
   "source": [
    "We will try to reproduce step by step the `piqa` benchmark with some slight modifications which consists of a multi-choice question on common-sense sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE0JB36uK0ko"
   },
   "source": [
    "Let's start to fill our `blank_yaml.yaml` file with these arguments\n",
    "\n",
    "```yaml\n",
    "task: piqa_modified\n",
    "dataset_path: baber/piqa\n",
    "dataset_name: null\n",
    "```\n",
    "\n",
    "- `task`: This refers to the task name\n",
    "- `dataset_path`: This refers to the name of the dataset on Hugging Face.\n",
    "- `dataset_name`: Leave `null` if your dataset does not require a config to be passed. See https://huggingface.co/docs/datasets/load_hub#configurations for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_uZp3dxMY-C"
   },
   "source": [
    "We will also set:\n",
    "\n",
    "```yaml\n",
    "output_type: multiple_choice\n",
    "```\n",
    "\n",
    "To make sure to set multi-choice question answering.\n",
    "\n",
    "We will also set the names of the training / validation and test split. For that make sure to inspect the split names within the Hugging Face dataset.\n",
    "\n",
    "```yaml\n",
    "training_split: train\n",
    "validation_split: validation\n",
    "test_split: null\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqQKX6ROWFe"
   },
   "source": [
    "\n",
    "```yaml\n",
    "doc_to_text: \"Question: {{goal}}\\nAnswer:\"\n",
    "doc_to_target: label\n",
    "doc_to_choice: \"{{[sol1, sol2]}}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAMPKPJqT1EU"
   },
   "source": [
    "Finally, we will add the following blocks so that lm-eval will compute the accuracy and normalized accuracy for this metric:\n",
    "\n",
    "```yaml\n",
    "metric_list:\n",
    "  - metric: acc\n",
    "    aggregation: mean\n",
    "    higher_is_better: true\n",
    "  - metric: acc_norm\n",
    "    aggregation: mean\n",
    "    higher_is_better: true\n",
    "metadata:\n",
    "  version: 1.0\n",
    "dataset_kwargs:\n",
    "  trust_remote_code: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_6xOfAST0fI",
    "outputId": "097f8590-49ee-41af-daf0-4a316753a2bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-04-11 00:47:10.150262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744332430.472127    3161 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744332430.556385    3161 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-11 00:47:11.215483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-11:00:47:21 INFO     [__main__:440] Selected Tasks: ['piqa_modified']\n",
      "2025-04-11:00:47:21 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-04-11:00:47:21 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': 'HuggingFaceTB/SmolLM2-135M', 'dtype': 'bfloat16'}\n",
      "2025-04-11:00:47:21 INFO     [models.huggingface:136] Using device 'cuda'\n",
      "config.json: 100% 704/704 [00:00<00:00, 5.06MB/s]\n",
      "tokenizer_config.json: 100% 3.66k/3.66k [00:00<00:00, 28.2MB/s]\n",
      "vocab.json: 100% 801k/801k [00:00<00:00, 1.07MB/s]\n",
      "merges.txt: 100% 466k/466k [00:00<00:00, 16.2MB/s]\n",
      "tokenizer.json: 100% 2.10M/2.10M [00:00<00:00, 11.8MB/s]\n",
      "special_tokens_map.json: 100% 831/831 [00:00<00:00, 5.13MB/s]\n",
      "2025-04-11:00:47:24 INFO     [models.huggingface:377] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "model.safetensors: 100% 269M/269M [00:00<00:00, 285MB/s]\n",
      "generation_config.json: 100% 111/111 [00:00<00:00, 488kB/s]\n",
      "piqa_train.parquet: 100% 2.64M/2.64M [00:00<00:00, 62.3MB/s]\n",
      "piqa_validation.parquet: 100% 300k/300k [00:00<00:00, 21.8MB/s]\n",
      "piqa_test.parquet: 100% 496k/496k [00:00<00:00, 142MB/s]\n",
      "Generating train split: 100% 16113/16113 [00:00<00:00, 187931.04 examples/s]\n",
      "Generating validation split: 100% 1838/1838 [00:00<00:00, 165102.49 examples/s]\n",
      "Generating test split: 100% 3084/3084 [00:00<00:00, 337452.61 examples/s]\n",
      "2025-04-11:00:47:30 INFO     [api.task:427] Building contexts for piqa_modified on rank 0...\n",
      "100% 1838/1838 [00:01<00:00, 1097.10it/s]\n",
      "2025-04-11:00:47:32 INFO     [evaluator:559] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0% 0/3676 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 64\n",
      "Running loglikelihood requests: 100% 3676/3676 [00:26<00:00, 141.14it/s]\n",
      "2025-04-11:00:48:03 INFO     [loggers.evaluation_tracker:209] Saving results aggregated\n",
      "hf (pretrained=HuggingFaceTB/SmolLM2-135M,dtype=bfloat16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)\n",
      "|    Tasks    |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|piqa_modified|      1|none  |     0|acc     |↑  |0.6839|±  |0.0108|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.6817|±  |0.0109|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch -m lm_eval --model hf \\\n",
    "    --model_args pretrained=HuggingFaceTB/SmolLM2-135M,dtype=bfloat16 \\\n",
    "    --tasks piqa_modified \\\n",
    "    --batch_size auto \\\n",
    "    --output_path results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl5n07ITUimf"
   },
   "source": [
    "For `multiple_choice` tasks, `lm-eval` will calculate the log-likelihood of all choices, and consider the answer with the highest log-likelihood as the chosen answer.\n",
    "\n",
    "It is also possible to create `generate_until` tasks which consists of generative tasks. For that, you can look more into details by navigating what is done for other tasks [such as `gsm8k`](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZOByjU4Vk3A"
   },
   "source": [
    "Let's now try to make our piqa benchmark slightly more different - we will replace the solution by their paraphrased versions. For that, we will use the tool [\"T5 Paraphrase Generation\"](https://huggingface.co/spaces/AventIQ-AI/t5-paraphrase-generation) to paraphrase the solutions.\n",
    "\n",
    "For that we will use `gradio_client` to use the Space as an API. Let's first test it on some examples of piqa:\n",
    "\n",
    "```python\n",
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"AventIQ-AI/t5-paraphrase-generation\")\n",
    "result = client.predict(\n",
    "\t\ttext=\"Hello!!\",\n",
    "\t\tmax_length=50,\n",
    "\t\ttemperature=1,\n",
    "\t\tapi_name=\"/predict\"\n",
    ")\n",
    "print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfwb7-9TUiZ7"
   },
   "outputs": [],
   "source": [
    "! pip install -q gradio_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvIsHpiC8L0i",
    "outputId": "eec22bcd-14c9-4dff-b5d6-a61f2af4c4b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://aventiq-ai-t5-paraphrase-generation.hf.space ✔\n",
      "['Etymologically, weld the metal together to ensure that it holds its firmly in place.', 'The metal was separated to ensure its stabilization and strong stability.', 'Wel\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"AventIQ-AI/t5-paraphrase-generation\")\n",
    "result = client.predict(\n",
    "\t\ttext=\"Weld the metal together to get it to stay firmly in place\",\n",
    "\t\tmax_length=50,\n",
    "\t\ttemperature=1,\n",
    "\t\tapi_name=\"/predict\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsmgq9ZGWsAn"
   },
   "source": [
    "As you can see, the tool generates N possible paraphrases, we can create an utility method that replaces the solutions with a sampled paraphrased sentence:\n",
    "\n",
    "```python\n",
    "import random\n",
    "import datasets\n",
    "\n",
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"AventIQ-AI/t5-paraphrase-generation\")\n",
    "\n",
    "def paraphrase(text):\n",
    "  result = client.predict(\n",
    "\t\ttext=text,\n",
    "\t\tmax_length=50,\n",
    "\t\ttemperature=1,\n",
    "\t\tapi_name=\"/predict\"\n",
    "  )\n",
    "  return random.choice(result)\n",
    "\n",
    "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n",
    "    def _process_doc(doc):\n",
    "        sol1 = paraphrase(doc[\"sol1\"])\n",
    "        sol2 = paraphrase(doc[\"sol2\"])\n",
    "        out_doc = {\n",
    "            \"goal\": doc[\"goal\"],\n",
    "            \"sol1\": sol1,\n",
    "            \"sol2\": sol2,\n",
    "            \"label\": doc[\"label\"],\n",
    "        }\n",
    "        return out_doc\n",
    "\n",
    "    return dataset.map(_process_doc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0N8LzK8Xxzb"
   },
   "source": [
    "We will perform the processing offline and save the modified dataset locally (just a fraction of it for demonstration purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117,
     "referenced_widgets": [
      "47e62c5648c340adb0646eceb3c39658",
      "d28c107cbe914118bf1a9c7c733736b7",
      "9fe32557185748ba9e34905c2a0ddf0c",
      "dfeaecb81c1044c1a17edbdaf66b0f77",
      "a4c7d98e576b4b6798eefcc5f041601e",
      "b3d03254c9cf417db8bdc9197a31654a",
      "cdedfbdb33a147eaa6e7a831f1107ffc",
      "234a6887466a40409d614517c7487414",
      "b0ccbfcc57ba4d8e84ce7a6605225c5b",
      "04c3f96ff56742cf8185a46aaa16835c",
      "0e64ed81b6b34e6b8c1df6de3f8c4999",
      "5560f6acad744ceb86cc17d483fd4087",
      "48900d56f57e48ff82aa414fa98fe4ae",
      "4a82cf1294954c78b35bbadb2fb680a5",
      "d02fd859f0084d82a7071ca233057cb1",
      "e6fafbb547f04cc1ad83c033fc8bd72f",
      "668216d1074546a7b01dc44ede67e7f5",
      "bf517cd64f1d40ae9088afee9d0a46f7",
      "d8b606801e834589878908ffe611964b",
      "4d5f6842c04f4028a8bedd0799d22163",
      "4e028cc9faea418ebd4690fe614b200d",
      "bacd0fd3b6094bb8b2aad9b471e8d089"
     ]
    },
    "id": "xxqzrLcWXxKr",
    "outputId": "d787ba28-6ca0-41de-f28e-5e26f6d109db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://aventiq-ai-t5-paraphrase-generation.hf.space ✔\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e62c5648c340adb0646eceb3c39658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5560f6acad744ceb86cc17d483fd4087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "38383"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import datasets\n",
    "import json\n",
    "\n",
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"AventIQ-AI/t5-paraphrase-generation\")\n",
    "\n",
    "def paraphrase(text):\n",
    "  result = client.predict(\n",
    "\t\ttext=text,\n",
    "\t\tmax_length=50,\n",
    "\t\ttemperature=1,\n",
    "\t\tapi_name=\"/predict\"\n",
    "  )\n",
    "  # A hack to convert properly the pure string into a list\n",
    "  result = result.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" '\", \"\").replace(\"['\", \"\").split(\"',\")\n",
    "  return random.choice(result).replace(\"'\", \"\")\n",
    "\n",
    "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n",
    "    def _process_doc(doc):\n",
    "        sol1 = paraphrase(doc[\"sol1\"])\n",
    "        sol2 = paraphrase(doc[\"sol2\"])\n",
    "        out_doc = {\n",
    "            \"goal\": doc[\"goal\"],\n",
    "            \"sol1\": sol1,\n",
    "            \"sol2\": sol2,\n",
    "            \"label\": doc[\"label\"],\n",
    "        }\n",
    "        return out_doc\n",
    "\n",
    "    return dataset.map(_process_doc)\n",
    "\n",
    "YOUR_HF_TOKEN = \"\" # Paste your HF Token here\n",
    "\n",
    "dataset = datasets.load_dataset(\"baber/piqa\", split='train[:1%]')\n",
    "dataset = process_docs(dataset)\n",
    "\n",
    "# we will also create a validation split\n",
    "dataset.push_to_hub(\"piqa_modified\", split=\"validation\", private=True, token=YOUR_HF_TOKEN)\n",
    "\n",
    "dataset = datasets.load_dataset(\"baber/piqa\", split='train[1%:2%]')\n",
    "dataset = process_docs(dataset)\n",
    "\n",
    "dataset.push_to_hub(\"piqa_modified\", split=\"validation\", private=True, token=YOUR_HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgAEVq5taCXJ"
   },
   "source": [
    "After that, make sure to modify the dataset paths correctly to point it to your namespace - in our case:\n",
    "\n",
    "```yaml\n",
    "dataset_path: tiiuae/piqa_modified\n",
    "dataset_name: null\n",
    "output_type: multiple_choice\n",
    "training_split: train\n",
    "validation_split: validation\n",
    "test_split: null\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJK77ysxXq1O",
    "outputId": "b7bde676-ca79-4a33-b699-83ae36fcd261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-04-11 02:06:43.969039: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744337203.992954   24010 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744337204.000605   24010 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-11 02:06:44.023280: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-11:02:06:51 INFO     [__main__:440] Selected Tasks: ['piqa_modified']\n",
      "2025-04-11:02:06:51 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-04-11:02:06:51 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': 'HuggingFaceTB/SmolLM2-135M', 'dtype': 'bfloat16'}\n",
      "2025-04-11:02:06:51 INFO     [models.huggingface:136] Using device 'cuda'\n",
      "2025-04-11:02:06:52 INFO     [models.huggingface:377] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "README.md: 100% 546/546 [00:00<00:00, 4.36MB/s]\n",
      "Generating train split: 100% 161/161 [00:00<00:00, 35890.67 examples/s]\n",
      "Generating validation split: 100% 161/161 [00:00<00:00, 75831.89 examples/s]\n",
      "2025-04-11:02:06:55 INFO     [api.task:427] Building contexts for piqa_modified on rank 0...\n",
      "100% 161/161 [00:00<00:00, 1137.75it/s]\n",
      "2025-04-11:02:06:55 INFO     [evaluator:559] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0% 0/322 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 64\n",
      "Running loglikelihood requests: 100% 322/322 [00:04<00:00, 76.48it/s] \n",
      "/content/lm-evaluation-harness/lm_eval/api/task.py:1577: RuntimeWarning: divide by zero encountered in divide\n",
      "  pred_norm = np.argmax(lls / completion_len)\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "2025-04-11:02:07:02 INFO     [loggers.evaluation_tracker:209] Saving results aggregated\n",
      "hf (pretrained=HuggingFaceTB/SmolLM2-135M,dtype=bfloat16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)\n",
      "|    Tasks    |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|piqa_modified|      1|none  |     0|acc     |↑  |0.5839|±  |0.0390|\n",
      "|             |       |none  |     0|acc_norm|↑  |0.5342|±  |0.0394|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!HF_TOKEN=\"YOUR_TOKEN\" accelerate launch -m lm_eval --model hf \\\n",
    "    --model_args pretrained=HuggingFaceTB/SmolLM2-135M,dtype=bfloat16 \\\n",
    "    --tasks piqa_modified \\\n",
    "    --batch_size auto \\\n",
    "    --output_path results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c19YblMMbDYX"
   },
   "source": [
    "Once you are happy with your new benchmark - refer to our submission guideline (we will detail the submission process soon) to submit your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZI7pRy-ajI-"
   },
   "source": [
    "For much detailed tutorial, make sure to read carefully [the guide from lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md) in order to have an extensive understanding of all possible corner cases and integrations."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04c3f96ff56742cf8185a46aaa16835c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e64ed81b6b34e6b8c1df6de3f8c4999": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "234a6887466a40409d614517c7487414": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47e62c5648c340adb0646eceb3c39658": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d28c107cbe914118bf1a9c7c733736b7",
       "IPY_MODEL_9fe32557185748ba9e34905c2a0ddf0c",
       "IPY_MODEL_dfeaecb81c1044c1a17edbdaf66b0f77"
      ],
      "layout": "IPY_MODEL_a4c7d98e576b4b6798eefcc5f041601e"
     }
    },
    "48900d56f57e48ff82aa414fa98fe4ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_668216d1074546a7b01dc44ede67e7f5",
      "placeholder": "​",
      "style": "IPY_MODEL_bf517cd64f1d40ae9088afee9d0a46f7",
      "value": "Creating json from Arrow format: 100%"
     }
    },
    "4a82cf1294954c78b35bbadb2fb680a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8b606801e834589878908ffe611964b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4d5f6842c04f4028a8bedd0799d22163",
      "value": 1
     }
    },
    "4d5f6842c04f4028a8bedd0799d22163": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e028cc9faea418ebd4690fe614b200d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5560f6acad744ceb86cc17d483fd4087": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48900d56f57e48ff82aa414fa98fe4ae",
       "IPY_MODEL_4a82cf1294954c78b35bbadb2fb680a5",
       "IPY_MODEL_d02fd859f0084d82a7071ca233057cb1"
      ],
      "layout": "IPY_MODEL_e6fafbb547f04cc1ad83c033fc8bd72f"
     }
    },
    "668216d1074546a7b01dc44ede67e7f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fe32557185748ba9e34905c2a0ddf0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_234a6887466a40409d614517c7487414",
      "max": 161,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0ccbfcc57ba4d8e84ce7a6605225c5b",
      "value": 161
     }
    },
    "a4c7d98e576b4b6798eefcc5f041601e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0ccbfcc57ba4d8e84ce7a6605225c5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3d03254c9cf417db8bdc9197a31654a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bacd0fd3b6094bb8b2aad9b471e8d089": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf517cd64f1d40ae9088afee9d0a46f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cdedfbdb33a147eaa6e7a831f1107ffc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d02fd859f0084d82a7071ca233057cb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e028cc9faea418ebd4690fe614b200d",
      "placeholder": "​",
      "style": "IPY_MODEL_bacd0fd3b6094bb8b2aad9b471e8d089",
      "value": " 1/1 [00:00&lt;00:00, 62.66ba/s]"
     }
    },
    "d28c107cbe914118bf1a9c7c733736b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3d03254c9cf417db8bdc9197a31654a",
      "placeholder": "​",
      "style": "IPY_MODEL_cdedfbdb33a147eaa6e7a831f1107ffc",
      "value": "Map: 100%"
     }
    },
    "d8b606801e834589878908ffe611964b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfeaecb81c1044c1a17edbdaf66b0f77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04c3f96ff56742cf8185a46aaa16835c",
      "placeholder": "​",
      "style": "IPY_MODEL_0e64ed81b6b34e6b8c1df6de3f8c4999",
      "value": " 161/161 [12:42&lt;00:00,  4.97s/ examples]"
     }
    },
    "e6fafbb547f04cc1ad83c033fc8bd72f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
