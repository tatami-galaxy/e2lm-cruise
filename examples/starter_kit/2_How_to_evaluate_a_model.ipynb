{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-XNyuDTi9hm"
   },
   "source": [
    "# 2- How to evaluate a model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlDL61VmjGdv"
   },
   "source": [
    "In this notebook, you will understand how to evaluate a checkpoint using the [`lm-evaluation-harness`](https://github.com/EleutherAI/lm-evaluation-harness) package. You will understand how to install and use `lm-eval` package to run evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOLYhpQ8vCjQ"
   },
   "source": [
    "## Selecting a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssTDUCfJvED9"
   },
   "source": [
    "To check available tasks, you can navigate [here](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks) and get all supported tasks by `lm-eval`.\n",
    "\n",
    "For the sake of demonstration, we will evaluate `HuggingFaceTB/SmolLM2-135M` on `hellaswag` benchmark.\n",
    "\n",
    "You can read more about the benchmark in the [original paper](https://arxiv.org/abs/1905.07830)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2Bo_O8AvieG"
   },
   "source": [
    "## Running the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw3riAyOk_07"
   },
   "source": [
    "To run an evaluation, follow this command line template. You can replace the model ID with something else by changing the `pretrained=` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Z4l1tcEj6bX",
    "outputId": "2ad597c7-af91-4d13-d5e7-8c8fdd098cd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\n",
      "2025-07-11:07:13:53 INFO     [__main__:441] Selected Tasks: ['hellaswag']\n",
      "2025-07-11:07:13:53 INFO     [evaluator:198] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-07-11:07:13:53 INFO     [evaluator:236] Initializing hf model, with arguments: {'pretrained': 'HuggingFaceTB/SmolLM2-135M', 'dtype': 'bfloat16'}\n",
      "2025-07-11:07:13:54 INFO     [models.huggingface:138] Using device 'cuda'\n",
      "config.json: 100%|█████████████████████████████| 704/704 [00:00<00:00, 2.21MB/s]\n",
      "tokenizer_config.json: 3.66kB [00:00, 7.11MB/s]\n",
      "vocab.json: 801kB [00:00, 3.72MB/s]\n",
      "merges.txt: 466kB [00:00, 13.1MB/s]\n",
      "tokenizer.json: 2.10MB [00:00, 19.0MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 831/831 [00:00<00:00, 3.76MB/s]\n",
      "2025-07-11:07:13:59 INFO     [models.huggingface:391] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "model.safetensors: 100%|█████████████████████| 269M/269M [00:03<00:00, 67.5MB/s]\n",
      "generation_config.json: 100%|███████████████████| 111/111 [00:00<00:00, 432kB/s]\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'hellaswag' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "2025-07-11:07:14:06 ERROR    [datasets.load:1367] `trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'hellaswag' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "README.md: 7.02kB [00:00, 9.22MB/s]\n",
      "data/train-00000-of-00001.parquet: 100%|███| 24.4M/24.4M [00:01<00:00, 18.5MB/s]\n",
      "data/test-00000-of-00001.parquet: 100%|████| 6.11M/6.11M [00:01<00:00, 5.05MB/s]\n",
      "data/validation-00000-of-00001.parquet: 100%|█| 6.32M/6.32M [00:01<00:00, 3.38MB\n",
      "Generating train split: 100%|██| 39905/39905 [00:00<00:00, 165011.07 examples/s]\n",
      "Generating test split: 100%|███| 10003/10003 [00:00<00:00, 186299.70 examples/s]\n",
      "Generating validation split: 100%|█| 10042/10042 [00:00<00:00, 177949.41 example\n",
      "Map: 100%|███████████████████████| 39905/39905 [00:06<00:00, 6150.11 examples/s]\n",
      "Map: 100%|███████████████████████| 10042/10042 [00:01<00:00, 9012.36 examples/s]\n",
      "2025-07-11:07:14:33 INFO     [api.task:434] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:02<00:00, 3537.04it/s]\n",
      "2025-07-11:07:14:37 INFO     [evaluator:568] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                 | 0/40168 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 64\n",
      "Running loglikelihood requests: 100%|███| 40168/40168 [00:38<00:00, 1033.91it/s]\n",
      "2025-07-11:07:15:32 INFO     [loggers.evaluation_tracker:209] Saving results aggregated\n",
      "hf (pretrained=HuggingFaceTB/SmolLM2-135M,dtype=bfloat16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (64)\n",
      "|  Tasks  |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|---------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|hellaswag|      1|none  |     0|acc     |↑  |0.3554|±  |0.0048|\n",
      "|         |       |none  |     0|acc_norm|↑  |0.4311|±  |0.0049|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch -m lm_eval --model hf \\\n",
    "    --model_args pretrained=HuggingFaceTB/SmolLM2-135M,dtype=bfloat16 \\\n",
    "    --tasks hellaswag \\\n",
    "    --batch_size auto \\\n",
    "    --output_path results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_EpUtpvw0Jr"
   },
   "source": [
    "Let's now inspect the result. We can specify the output path with `output_path` parameter. Inside that path you should have a json file with a time-stamp corresponding to the moment the model has been evaluated.\n",
    "\n",
    "You can navigate into the json file and retrieve the field `results` to get the scores. In the case of `hellaswag`, it is possible to get `acc` (accuracy) and `acc_norm` (normalizaed accuracy).\n",
    "\n",
    "Note that the results are also directly displayed into the terminal.\n",
    "\n",
    "```json\n",
    "  \"results\": {\n",
    "    \"hellaswag\": {\n",
    "      \"alias\": \"hellaswag\",\n",
    "      \"acc,none\": 0.3545110535749851,\n",
    "      \"acc_stderr,none\": 0.004773872456201056,\n",
    "      \"acc_norm,none\": 0.4311890061740689,\n",
    "      \"acc_norm_stderr,none\": 0.004942302768002102\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIQh1RVlwP-0"
   },
   "source": [
    "If you would like to change the number of shots, you can also pass the `--num_fewshot` parameter. For example, if you want to run `hellaswag` on 25-shots, you can run the following command line argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "anL2hYR8wfdK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\n",
      "2025-07-11:07:16:34 INFO     [__main__:441] Selected Tasks: ['hellaswag']\n",
      "2025-07-11:07:16:34 INFO     [evaluator:198] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-07-11:07:16:34 INFO     [evaluator:236] Initializing hf model, with arguments: {'pretrained': 'HuggingFaceTB/SmolLM2-135M', 'dtype': 'bfloat16'}\n",
      "2025-07-11:07:16:34 INFO     [models.huggingface:138] Using device 'cuda'\n",
      "2025-07-11:07:16:35 INFO     [models.huggingface:391] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'hellaswag' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "2025-07-11:07:16:36 ERROR    [datasets.load:1367] `trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'hellaswag' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "2025-07-11:07:16:49 WARNING  [evaluator:318] Overwriting default num_fewshot of hellaswag from None to 25\n",
      "2025-07-11:07:16:49 INFO     [api.task:434] Building contexts for hellaswag on rank 0...\n",
      "100%|█████████████████████████████████████| 10042/10042 [02:01<00:00, 82.75it/s]\n",
      "2025-07-11:07:18:51 INFO     [evaluator:568] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                 | 0/40168 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Determined largest batch size: 16\n",
      "Running loglikelihood requests: 100%|█████| 40168/40168 [13:54<00:00, 48.11it/s]\n",
      "2025-07-11:07:37:46 INFO     [loggers.evaluation_tracker:209] Saving results aggregated\n",
      "hf (pretrained=HuggingFaceTB/SmolLM2-135M,dtype=bfloat16), gen_kwargs: (None), limit: None, num_fewshot: 25, batch_size: auto (16)\n",
      "|  Tasks  |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|---------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|hellaswag|      1|none  |    25|acc     |↑  |0.3538|±  |0.0048|\n",
      "|         |       |none  |    25|acc_norm|↑  |0.4406|±  |0.0050|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch -m lm_eval --model hf \\\n",
    "    --model_args pretrained=HuggingFaceTB/SmolLM2-135M,dtype=bfloat16 \\\n",
    "    --tasks hellaswag \\\n",
    "    --batch_size auto \\\n",
    "    --output_path results_25_shots/ \\\n",
    "    --num_fewshot 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP1JTxhowpyg"
   },
   "source": [
    "Since the evaluation framework uses `accelerate` library, you can also use this notebook in a Kaggle multi-GPU notebook to benefit from multi-GPU inference and make the evaluation faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCuOi30ryEvM"
   },
   "source": [
    "## Going further\n",
    "\n",
    "This notebook simply demonstrates how to run a simple evaluation using common parameters which we believe should be sufficient enough for the competition. Feel free to check out the [official documentation page](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
