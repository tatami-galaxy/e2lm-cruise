{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Scientific alignment"
      ],
      "metadata": {
        "id": "uC-593PZ7JDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide here the script we will use to check the scientific alignment of created benchmarks for reproducibility purpose.\n",
        "\n",
        "First, make sure that you have all pairs of question answers stored in a `json` file with the following format:\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"content\": \"Question: Typical advertising regulatory bodies suggest, for example that adverts must not: encourage _________, cause unnecessary ________ or _____, and must not cause _______ offence.\\nA) Safe practices, Fear, Jealousy, Trivial\\nB) Unsafe practices, Distress, Joy, Trivial\\nC) Safe practices, Wants, Jealousy, Trivial\\nD) Safe practices, Distress, Fear, Trivial\\nE) Unsafe practices, Wants, Jealousy, Serious\\nF) Safe practices, Distress, Jealousy, Serious\\nG) Safe practices, Wants, Fear, Serious\\nH) Unsafe practices, Wants, Fear, Trivial\\nI) Unsafe practices, Distress, Fear, Serious\\nAnswer: I\"\n",
        "  },\n",
        "  {\n",
        "    \"content\": \"Question: Managers are entrusted to run the company in the best interest of ________. Specifically, they have a duty to act for the benefit of the company, as well as a duty of ________ and of _______.\\nA) Shareholders, Diligence, Self-interest\\nB) Shareholders, Self-interest, Care and Skill\\nC) Stakeholders, Care and skill, Self-interest\\nD) Stakeholders, Diligence, Care and Skill\\nE) Customers, Care and Skill, Diligence\\nF) Shareholders, Care and Skill, Diligence\\nG) Shareholders, Self-interest, Diligence\\nH) Employees, Care and Skill, Diligence\\nI) Stakeholders, Self-interest, Diligence\\nJ) Stakeholder, Care and Skill, Diligence\\nAnswer: F\"\n",
        "  }\n",
        "  ...\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "K8acOMFp7fB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use OpenAI's GPT-4o as a judge, make sure to use your OpenAI API key and run the script below:"
      ],
      "metadata": {
        "id": "fTaqYG168JPC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWRAfg5a67vQ"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "process_function = {\n",
        "    \"name\": \"classify_prompt\",\n",
        "    \"description\": \"Classify a prompt as STEM/Math (Accept) or General NLP (Reject).\",\n",
        "    \"schema\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"verdict\": {\"type\": \"string\", \"enum\": [\"Accept\", \"Reject\"]}\n",
        "        },\n",
        "        \"required\": [\"verdict\"],\n",
        "        \"additionalProperties\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def read_jsonl(jsonl_file_path):\n",
        "    \"\"\"\n",
        "    Read a .jsonl file into a Python list of dictionaries (or other JSON-serializable objects).\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    with open(jsonl_file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:  # skip empty lines\n",
        "                record = json.loads(line)\n",
        "                records.append(record)\n",
        "    return records\n",
        "\n",
        "def read_json(json_file_path):\n",
        "    \"\"\"\n",
        "    Read a JSON file into a Python object.\n",
        "    \"\"\"\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        records = json.load(f)\n",
        "    return records\n",
        "\n",
        "def classify_prompts(json_path, num_samples=50, output_file=\"prompt_classifications.json\"):\n",
        "    \"\"\"\n",
        "    Sample question-answer pairs from a JSON file and classify them using GPT-4o.\n",
        "\n",
        "    Args:\n",
        "        json_path: Path to the JSON file containing question-answer pairs\n",
        "        num_samples: Number of question-answer pairs to sample for classification\n",
        "        output_file: Path to save the classification results\n",
        "    \"\"\"\n",
        "    # Load all question-answer pairs from the JSON file\n",
        "    qa_pairs = read_json(json_path)\n",
        "\n",
        "    # Sample a subset of question-answer pairs\n",
        "    if len(qa_pairs) > num_samples:\n",
        "        sampled_qa_pairs = random.sample(qa_pairs, num_samples)\n",
        "    else:\n",
        "        sampled_qa_pairs = qa_pairs\n",
        "        print(f\"Warning: Requested {num_samples} samples but only {len(qa_pairs)} question-answer pairs available.\")\n",
        "\n",
        "    # Create the classification prompt\n",
        "    classification_prompt = \"\"\"You are tasked with classifying each of the following question-answer pairs into one of two groups:\n",
        "    ---\n",
        "    ### Accept\n",
        "\n",
        "    Classify as **Accept** if the question requires **domain-specific knowledge**, **scientific understanding**, **academic expertise**, or **professional reasoning**. This includes but is not limited to:\n",
        "\n",
        "    **Scientific & Technical Fields**\n",
        "    - Biology\n",
        "    - Chemistry\n",
        "    - Physics\n",
        "    - Mathematics\n",
        "    - Computer Science\n",
        "    - Engineering (all disciplines)\n",
        "    - Medicine and Health Sciences\n",
        "    - Neuroscience\n",
        "    - Pharmacology\n",
        "    - Veterinary Science\n",
        "    - Environmental Science\n",
        "    - Earth Science / Astronomy\n",
        "    - Statistics & Data Science\n",
        "\n",
        "    **Professional & Applied Domains**\n",
        "    - Law\n",
        "    - Business (e.g., Finance, Accounting, Marketing)\n",
        "    - Economics\n",
        "    - Political Science\n",
        "    - Education\n",
        "    - Sociology\n",
        "    - Anthropology\n",
        "    - Linguistics\n",
        "    - Communications / Media Studies\n",
        "    - Library & Information Science\n",
        "    - Social Work\n",
        "    - Public Policy\n",
        "    - Nursing / Allied Health\n",
        "    - Architecture / Urban Planning\n",
        "    - Agriculture / Food Science\n",
        "\n",
        "    **Humanities with Reasoning Requirements**\n",
        "    - History\n",
        "    - Philosophy (e.g., logic, ethics, epistemology)\n",
        "    - Theology / Religious Studies\n",
        "    - Art History\n",
        "    - Literary Theory\n",
        "\n",
        "    Any question that falls under these or similar knowledge-intensive domains should be marked **Accept**, even if it's not explicitly listed.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Reject\n",
        "\n",
        "    Classify as **Reject** only if the question is based on:\n",
        "    - General language understanding\n",
        "    - Common sense or cultural knowledge\n",
        "    - Vocabulary, grammar, spelling, or idioms\n",
        "    - Word analogies or word associations\n",
        "    - Trivia or factoids that donâ€™t require reasoning\n",
        "    - Sentiment, emotion, or tone recognition\n",
        "    - Simple reading comprehension without technical content\n",
        "    - NLP-specific tasks (e.g., joke detection, paraphrasing, summarization)\n",
        "\n",
        "    These do not require deep subject-matter knowledge and should be marked **Reject**.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Output Format\n",
        "    ```json\n",
        "    {\n",
        "      \"verdict\": \"Accept\" // or \"Reject\"\n",
        "    }\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    # Process each question-answer pair individually for simpler output structure\n",
        "    results = []\n",
        "\n",
        "    for i, qa_pair in enumerate(sampled_qa_pairs):\n",
        "        print(f\"Processing QA pair {i+1}/{len(sampled_qa_pairs)}...\")\n",
        "\n",
        "        # Extract question and answer from the pair\n",
        "        question_answer = qa_pair.get('content', '')\n",
        "\n",
        "        # Create prompt-specific classification request\n",
        "        current_prompt = f\"{classification_prompt}\\n{question_answer}\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert educational content classifier specializing in identifying STEM, Math, and general language tasks.\"},\n",
        "            {\"role\": \"user\", \"content\": current_prompt}\n",
        "        ]\n",
        "\n",
        "        response_format = {\n",
        "            \"type\": \"json_schema\",\n",
        "            \"json_schema\": process_function,\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-4o-2024-08-06\",\n",
        "                messages=messages,\n",
        "                max_tokens=300,\n",
        "                temperature=0.0,\n",
        "                response_format=response_format\n",
        "            )\n",
        "\n",
        "            # Extract and parse the JSON response\n",
        "            classification_result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "            # Add question-answer and ID to the result\n",
        "            result_with_qa = {\n",
        "                \"qa_id\": i+1,\n",
        "                \"content\": question_answer,\n",
        "                \"verdict\": classification_result[\"verdict\"]\n",
        "            }\n",
        "\n",
        "            results.append(result_with_qa)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing QA pair {i+1}: {e}\")\n",
        "            results.append({\n",
        "                \"qa_id\": i+1,\n",
        "                \"content\": question_answer,\n",
        "                \"verdict\": \"Error\"\n",
        "            })\n",
        "\n",
        "    acc = 0\n",
        "    for item in results:\n",
        "        if item['verdict'].lower() == 'accept':\n",
        "            acc += 1\n",
        "    print(f\"Accuracy: {acc/len(results)}\")\n",
        "\n",
        "    output_data = {\"classifications\": results}\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(output_data, f, indent=2)\n",
        "\n",
        "    print(f\"Classification complete! Results saved to {output_file}\")\n",
        "    return output_data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    import os\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Classify prompts as STEM, Math, or General NLP tasks')\n",
        "    parser.add_argument('--input_json', type=str, default='./hellaswag_formatted.json', help='Path to the input JSON file containing prompts')\n",
        "    parser.add_argument('--num_samples', type=int, default=50, help='Number of prompts to sample for classification')\n",
        "    parser.add_argument('--output_file', type=str, default='prompt_classifications_hellaswag.json', help='Path to save the classification results')\n",
        "    parser.add_argument('--openai_key', type=str, help='OpenAI API key')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    os.environ[\"OPENAI_API_KEY\"] = args.openai_key\n",
        "    classify_prompts(\n",
        "        json_path=args.input_json,\n",
        "        num_samples=args.num_samples,\n",
        "        output_file=args.output_file\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final result will be saved in `output_file`."
      ],
      "metadata": {
        "id": "nL-Shqk88emv"
      }
    }
  ]
}